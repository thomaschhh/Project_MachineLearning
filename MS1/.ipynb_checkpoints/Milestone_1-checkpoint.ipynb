{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.cluster.vq import kmeans, whiten\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "#import clustering\n",
    "import Models\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def load_data(settings, batchSize, transformation=None, train_ratio=0.8, test_ratio=0.2):\n",
    "    \n",
    "    # preprocessing of data\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    tra = [transforms.Resize(256),\n",
    "           transforms.CenterCrop(224),\n",
    "           transforms.ToTensor(),\n",
    "           normalize]\n",
    "    \n",
    "    dataset = datasets.ImageFolder(settings, transform=transforms.Compose(tra))\n",
    "    \n",
    "    n_train = int(train_ratio * len(dataset))\n",
    "    n_test = int(test_ratio * len(dataset))\n",
    "    \n",
    "    data_train, data_test = torch.utils.data.random_split(dataset, [n_train, n_test])\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(data_train,\n",
    "                                             batch_size=batchSize,\n",
    "                                             pin_memory=True)\n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(data_test,\n",
    "                                             batch_size=batchSize,\n",
    "                                             pin_memory=True)\n",
    "    \n",
    "    return trainloader, testloader\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_assign(images_lists, dataset):\n",
    "    \"\"\"Creates a dataset from clustering, with clusters as labels.\n",
    "    Args:\n",
    "        images_lists (list of list): for each cluster, the list of image indexes\n",
    "                                    belonging to this cluster\n",
    "        dataset (list): initial dataset\n",
    "    Returns:\n",
    "        ReassignedDataset(torch.utils.data.Dataset): a dataset with clusters as\n",
    "                                                     labels\n",
    "    \"\"\"\n",
    "    assert images_lists is not None\n",
    "    pseudolabels = []\n",
    "    image_indexes = []\n",
    "    for cluster, images in enumerate(images_lists):\n",
    "        image_indexes.extend(images)\n",
    "        pseudolabels.extend([cluster] * len(images))\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    t = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                            transforms.RandomHorizontalFlip(),\n",
    "                            transforms.ToTensor(),\n",
    "                            normalize])\n",
    "\n",
    "    return ReassignedDataset(image_indexes, pseudolabels, dataset, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(ds, k, random_state=42):\n",
    "    \"\"\"\n",
    "    Create random cluster centroids.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : numpy array\n",
    "        The dataset to be used for centroid initialization.\n",
    "    k : int\n",
    "        The desired number of clusters for which centroids are required.\n",
    "    Returns\n",
    "    -------\n",
    "    centroids : numpy array\n",
    "        Collection of k centroids as a numpy array.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    centroids = []\n",
    "    m = np.shape(ds)[0]\n",
    "\n",
    "    for _ in range(k):\n",
    "        r = np.random.randint(0, m-1)\n",
    "        centroids.append(ds[r])\n",
    "\n",
    "    return np.array(centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(model, features):\n",
    "    #pca\n",
    "    pca_reduced = PCA(features, n_components=256)\n",
    "    #whitening\n",
    "    whitened=whiten(features)\n",
    "    #l2 normalization\n",
    "    f_normalized = normalize(whitened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(pre_data, k = 2):\n",
    "    random_cen = random(pre_data,k)\n",
    "    clustered_data = kmeans(pre_data,random_cen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataLoader, model, crit, optimizer, epoch):\n",
    "    for i, (input_tensor, target) in enumerate(dataLoader):\n",
    "        # switch to train mode\n",
    "        model.train()\n",
    "        # create an optimizer for the last fc layer\n",
    "        optimizer_tl = torch.optim.SGD(\n",
    "            model.top_layer.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=10**wd,\n",
    "        )\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input_tensor.cuda())\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        output = model(input_var)\n",
    "        loss = crit(output, target_var)\n",
    "\n",
    "        # record loss\n",
    "        losses.update(loss.data[0], input_tensor.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        opt.zero_grad()\n",
    "        optimizer_tl.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        optimizer_tl.step()\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main method\n",
    "def main(lr = 0.05, sobel = True, momentum = 0.9, wd = -5, ep = 10, bs = 2, k = 10):\n",
    "    \n",
    "    # load the data\n",
    "    trainL, testL = load_data(path, 1)\n",
    "    #load vgg\n",
    "    model = models.__dict__[\"vgg16\"](sobel) #choose classes?\n",
    "    fd = int(model.top_layer.weight.size()[1]) #what is that?\n",
    "    model.top_layer = None # why?\n",
    "    model.features = torch.nn.DataParallel(model.features)\n",
    "    model.cuda()\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # create optimizer\n",
    "    optimizer = torch.optim.SGD(\n",
    "            filter(lambda x: x.requires_grad, model.parameters()),\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            weight_decay=10**wd,\n",
    "       )\n",
    "\n",
    "    # define loss function\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    \n",
    "    # for all epochs\n",
    "    for epoch in range(ep):\n",
    "        # remove head\n",
    "        model.top_layer = None\n",
    "        model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "        # get the features for the whole dataset\n",
    "        features = compute_features(dataloader, model, len(dataset))\n",
    "        pre_data = preprocessing(model, features)\n",
    "        clus_data = clustering(pre_data)\n",
    "       \n",
    "        # pseudo labels\n",
    "        train_dataset = cluster_assign(clus_data, dataset.imgs)\n",
    "\n",
    "        # uniformly sample per target\n",
    "        sampler = UnifLabelSampler(int(args.reassign * len(train_dataset)),\n",
    "                                   clus_data)\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=args.batch,\n",
    "            num_workers=args.workers,\n",
    "            sampler=sampler,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        \n",
    "        # set last fully connected layer\n",
    "        mlp = list(model.classifier.children())\n",
    "        mlp.append(nn.ReLU(inplace=True).cuda())\n",
    "        model.classifier = nn.Sequential(*mlp)\n",
    "        model.top_layer = nn.Linear(fd, len(deepcluster.images_lists))\n",
    "        model.top_layer.weight.data.normal_(0, 0.01)\n",
    "        model.top_layer.bias.data.zero_()\n",
    "        model.top_layer.cuda()\n",
    "\n",
    "        # train network with clusters as pseudo-labels\n",
    "      \n",
    "                \n",
    "        loss = train(train_dataloader, model, criterion, optimizer, epoch)\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Das System kann den angegebenen Pfad nicht finden: '/home/space/datasets/imagenet/2012/train_set_small'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-44d6d6b5274b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/home/space/datasets/imagenet/2012/train_set_small'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrainL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-e2012a610a96>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(settings, batchSize, transformation, train_ratio, test_ratio)\u001b[0m\n\u001b[0;32m     23\u001b[0m            normalize]\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtra\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mn_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ratio\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    203\u001b[0m     def __init__(self, root, transform=None, target_transform=None,\n\u001b[0;32m    204\u001b[0m                  loader=default_loader, is_valid_file=None):\n\u001b[1;32m--> 205\u001b[1;33m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n\u001b[0m\u001b[0;32m    206\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m     92\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[0;32m     93\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[1;34m(self, dir)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mNo\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \"\"\"\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Das System kann den angegebenen Pfad nicht finden: '/home/space/datasets/imagenet/2012/train_set_small'"
     ]
    }
   ],
   "source": [
    "path = '/home/space/datasets/imagenet/2012/train_set_small'\n",
    "    \n",
    "\n",
    "for item in tqdm(trainL):\n",
    "    print(item)\n",
    "    plt.imshow(item[0].permute(2, 3, 1, 0).numpy().reshape(224, 224, 3))\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(data, outfile):\n",
    "    \n",
    "    # save a image using extension \n",
    "    image = image.save(\"geeks.jpg\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Protoype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "        end = time.time()\n",
    "\n",
    "        # remove head\n",
    "        model.top_layer = None\n",
    "        model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "\n",
    "        # get the features for the whole dataset\n",
    "        features = compute_features(dataloader, model, len(dataset))\n",
    "\n",
    "        # cluster the features\n",
    "        if args.verbose:\n",
    "            print('Cluster the features')\n",
    "        clustering_loss = deepcluster.cluster(features, verbose=args.verbose)\n",
    "\n",
    "        # assign pseudo-labels\n",
    "        if args.verbose:\n",
    "            print('Assign pseudo labels')\n",
    "        train_dataset = clustering.cluster_assign(deepcluster.images_lists,\n",
    "                                                  dataset.imgs)\n",
    "\n",
    "        # uniformly sample per target\n",
    "        sampler = UnifLabelSampler(int(args.reassign * len(train_dataset)),\n",
    "                                   deepcluster.images_lists)\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=args.batch,\n",
    "            num_workers=args.workers,\n",
    "            sampler=sampler,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        # set last fully connected layer\n",
    "        mlp = list(model.classifier.children())\n",
    "        mlp.append(nn.ReLU(inplace=True).cuda())\n",
    "        model.classifier = nn.Sequential(*mlp)\n",
    "        model.top_layer = nn.Linear(fd, len(deepcluster.images_lists))\n",
    "        model.top_layer.weight.data.normal_(0, 0.01)\n",
    "        model.top_layer.bias.data.zero_()\n",
    "        model.top_layer.cuda()\n",
    "\n",
    "        # train network with clusters as pseudo-labels\n",
    "        end = time.time()\n",
    "        loss = train(train_dataloader, model, criterion, optimizer, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
